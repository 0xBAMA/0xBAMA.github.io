%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.4 (15/5/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com) with extensive modifications by
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside,twocolumn,10pt]{article}

\usepackage{blindtext}

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage[hmarginratio=1:1,top=30mm,columnsep=18pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{amsmath}  % some math features, specifically using it for align

\usepackage{graphicx}
\graphicspath{ {./images/} }


% 
\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Voraldo v1.1 $\bullet$ November 2020} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
%% \usepackage{hyperref} % For hyperlinks in the PDF

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{Voraldo v1.1: \\\large A GPU-Based Voxel Editor with Support for Volumetric Lighting Effects} % Article title
\author{%
\textsc{Jonathan A. Baker}\thanks{\url{https://jbaker.graphics/} for more information.} \\[1ex] % Your name
\normalsize Ohio University \\ % Your institution
\normalsize \href{mailto:jb239812@ohio.edu}{jb239812@ohio.edu} % Your email address
%\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
%\textsc{Jane Smith}\thanks{Corresponding author} \\[1ex] % Second author's name
%\normalsize University of Utah \\ % Second author's institution
%\normalsize \href{mailto:jane@smith.com}{jane@smith.com} % Second author's email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
\begin{abstract}
  \noindent Volume graphics in realtime applications had seen some development in the 1990's before being largely superceded by rasterized polygons due to memory limitations. Today, commodity graphics hardware provides large, fast memory buffers and a highly parallel model for computation. This paper presents a demonstration of a realtime rendering method which traces rays through RGBA volume data for each pixel, with volumetric lighting. In order to generate something of substance to display, there are modeling and lighting features which are evaluated for each voxel using compute shaders.
\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}

\lettrine[nindent=0em,lines=3]{V} oxels, or volume elements, are a discrete means of representing volumes -- here, they are arranged orthogonally, and can be thought of as a 3 dimensional raster. A single value, or set of values, exists and is stored for each of these volume elements. This value can be associated with any point in the space defined by the voxel's extents. This has seen applications in physical simulation as well as computer graphics. 

One application which has provided the inspriation for this project and the series of projects which preceded it is realtime graphics, specifically within the domain of realtime images produced by video games and other interactive software. Early in the 1990s, there was contention on best practices for realtime hardware. As practices homogenized, polygonal graphics became the de-facto standard which would be used to produce all realtime 3D graphics. The math governing their use is well understood. Transforms exist which allow arbitrary rotation, scaling, shearing, flipping and many other types of operations to be performed on models defined as a series of triangles. Due to the simplicity of this shape -- three vertices -- there is an unambiguous definition of geometry: these three points define a plane, so long as they do not lie on the same line.

Some competing technologies existed, though many have largely fallen out of use. Voxels are one example of this. It is common today to see voxels in use, but until very recently \cite{VoxelGI}, not as a realtime rendering technology. What is frequently referred to as voxels today are in fact representations of voxels, rendered as polygons. This may sound like a pedantic distinction, so I will explain my reasoning.


%------------------------------------------------

\section{Background}

During the 1990's, there were a number of video games which were released that made use of voxels as a rendering technology. This list is not complete. Within these games there are a diverse set of approaches to getting an image on the screen.

\begin{itemize}
\item Commanche (Novalogic, 1992) 
\item Blade Runner (Westwood, 1997)
\item Shadow Warrior (3DRealms, 1997)
\item Blood (Monolith, 1997) 
\item Hexplore (Heliovision, 1998)
\item Outcast (Appeal, 1999)
\item RollerCoaster Tycoon (Chris Sawyer, 1999)
\item Command and Conquer: Tiberian Sun (Westwood, 1999)
\item Dwarf Fortress (Bay 12, 2006)
\end{itemize}

More recently, with approaches like marching cubes \cite{MarchingCubes}\footnote{Originally developed for visualization of medical data like CAT, MRI and PET scans.} and dual contouring \cite{DualContour} being applied to realtime applications, there exist relatively general methods to convert from a volume representation to a polygonal surface representation. So why consider voxels as a rendering technology at all? I hope to answer this question to the reader's satisfaction in the following sections.

As mentioned previously, v1.1 is not the first project I have done before arriving at this approach\footnote{See \url{https://jbaker.graphics/writings/voraldo_history.html} for more information on older approaches.}. Initially, this was an offline process, which would produce an image to a file. Once I was introduced to OpenGL, I had taken a number of different approaches using the existing geometric primitives.

First, I used simple points with static contents defined as vertex attributes before graduating to a representation with voxel contents evaluated in the vertex shader. These shared the limitation of relying upon OpenGL's order-dependent alpha blending, and would create severe artifacts when viewed in certain orientations. I explored solutions to this problem, such as Inigo Quilez's Volumetric Sort \cite{VolSort}\footnote{This keeps many copies of the data, in different orderings, and swaps out which is being used for display when back-to-front ordering would be violated, due to rotation.} but eventually moved to the use of CPU-generated 3D textures sampled by polygonal slices. In this case, maintaining front-to-back ordering is much simpler but when viewed side-on the polygons disappear.

To overcome these limitations, and to move more computation to the GPU, I stopped using OpenGL primitives at all and explored the use of the 4.2 Image Load/Store functionality \cite{ImgLoadStoreWiki}, which provides a GLSL interface for read/write access where texture samplers provide read-only access. Through a combination of this and the use of two triangles to cover the screen \cite{RWWTT}, the fragment shader became a very good place to composite samples through this volume\footnote{This led to the display method used in Voraldo v1.0 -- for more information, see \url{https://jbaker.graphics/writings/voraldo.html}.}.

%------------------------------------------------

\section{Methods}
\subsection{Rendering}

From my first use of Image Load/Store, I've discovered many applications for it -- for example, a two-fold optimization came out of my use of a rectangular texture to serve as a framebuffer. First, I can decouple the screen resolution from the resolution of the raycast operation. There is a scale factor which is defined at compile time, which can be set to a number greater than one to provide more samples or less than one to provide less samples per pixel for more limited hardware, such as an integrated GPU in a laptop. The combination of a flexible tile-based raycast renderer done in a compute shader, linear texture filtering and a supersampling buffer enables this approach to scale very well. The second part of this optimization is that this framebuffer texture only needs to be updated when something has happened to the volume data being displayed. Examples of this are drawing, lighting changes, rotation and scaling, which neccesitate an update to the displayed data. When a framebuffer update is called for, the compute shader performing the raycasting operation is invoked -- otherwise, all that is neccesary to present a frame is a simple blitting operation.

The rendering method itself is inspired by early approaches to 3D -- I believe the most appropriate term is raycasting. Starting with a unit vector pointing down the Z-axis, two user-controlled rotations are applied \cite{ArbitraryRotation}. An origin for each pixel can be calculated from the rotated basis vectors, with a starting location determined by pixel location and a direction established by the rotated Z vector. Once these two vectors are established, ray origin and direction, I can test the ray against an AABB used to represent the space containing the voxels to determine if this pixel needs to proceed \cite{RayBox}. If the ray hits, near and far intersection points are estalblished in the form of two floating point numbers, \texttt{tnear} and \texttt{tfar}. By sampling through the volume from the farthest intersection point to the nearest intersection point, simple `over' style alpha blending can be performed \cite{AlphaBlending}. Using the raw alpha channel proved to be limiting\footnote{GLSL normalizes these 8-bit values, and that assumption is carried through this paper -- when referencing a texture, the value returned is the value stored in memory, divided by 256. The alpha power is applied to this floating point number, in the range 0.0--1.0.} as very little of the range was usable. To counter this, the value held in the alpha channel is raised to a user-adjustable power to get the number that will be used in the computation.


In this computation, two buffers are referenced -- one four-channel image3D which determines the current color and opacity, and a single-channel image3D containing the current lighting data. The value in the lighting buffer is used to scale the intensity of the R, G and B color channels' contributions to the blending operation. The pixel color, C, is initialized with the OpenGL clear color, and is updated using a color sample, T, and a light sample, L, according to the following recurrence relation:
\begin{align*}
 C_{n+1}.rgb &= (T.rgb \cdot 4.0 \cdot L) \cdot T.a^{\alpha power} \\
 &+ C_{n}.rgb \cdot C_{n}.a \cdot (1 - T.a^{\alpha power})
\end{align*}
\begin{align*}
 C_{n+1}.a &= T.a^{\alpha power} + C_{n}.a \cdot (1 - T.a^{\alpha power})
\end{align*}

A series of samples is composited between the intersection points, based on some given step size and a maximum number of allowed steps. The final value of C.rgb is stored in the framebuffer texture.

The factor of four was determined experimentally to give a usable range, with enough headroom to create highlights. A value of zero will zero out the color contribution of the voxel under consideration. A value of 0.25 will cancel the scale factor to represent a neutral level of lighting. Higher values will begin to saturate at values of 1.0 for all three channels, making the contribution go to white\footnote{Note that if there are small values or zeroes in the R, G or B channels, it will not go to white, but it will eventually move towards saturation of the nonzero channels.}. The use of this separate lighting buffer is to decouple the logic governing the display from the logic governing the computation of the volumetric lighting.

\subsection{Modeling}

There are a number of moving parts to the modeling operation, in order to provide smooth interaction to the user and handle GPU syncronization. There are two sets of buffers involved: a 4-channel color and a 1-channel mask. Inspired by the double-buffering technique used by most graphics APIs, I keep two copies of each. This ensures that for any given drawing operation, I am reading from one texture representing the current state of the data and writing to the other, representing the new state of the data. The display code only references the current color buffer -- with appropriate use of memory barriers, and due to the single-threaded nature of OpenGL, there is no ambiguity as to which is which.

The drawing functions handle a number of different primitives, more than is appropriate to list here. They fall into a couple of classes - those which use a boolean \texttt{is\_inside()} function to determine if a given voxel's location is inside of the shape and those which use a separate load buffer to get data from the CPU. Both of these treat the mask the same way. As a 1-bit value, it determines whether or not that voxel can be written to -- if a cell is masked, it will retain its contents, even if it would be affected by the modeling functions. The shader associated with each drawing operation has independent toggles for drawing and masking. I have also included several functions to manipulate this mask, such as inverting, clearing, or masking ranges in the color channels\footnote{E.g. mask all voxels with a value in the red channel greater than 0.5.}.

The functions which use the \texttt{is\_inside()} function could easily be extended to handle arbitrary signed distance fields. I have not identified the best way to handle this interface yet, but this would be a good place to incorporate in-engine scripting to define an arbitrary \texttt{is\_inside()} function. There are only a few load buffer functions - loading of voxel data from a PNG image on disk and Brent Werness' Voxel Automata Terrain (VAT) \cite{VoxelAutomataTerrain}. VAT is an algorithm I converted from the original Processing implementation to C++. It generates highly varied voxel forms based on a sort of 3D analog to the Diamond-Square algorithm. This has been very useful in testing the implementation of my lighting functions. In both of these cases, the data is acquired and buffered to the GPU, before a compute shader is invoked to copy the data with a toggle to either respect or ignore the state of the mask.

The combination of drawing and masking functions has proved to be a very powerful way to produce complex voxel models. By keeping this 3D raster data on the GPU and manipulating its contents with these various compute shaders, many operations can be applied over one another in interesting and creative ways.


\subsection{Lighting}
%point, directional, ambient occlusion, global illumination, mash 
The lighting concepts are where we begin to see more benefit from the 3D raster. By using this representation, it becomes easier to talk about what is happening along a ray which traverses the volume. At any point along the ray's length the volume data can be queried to determine the opacity of the space.

There are a number of lighting functions that are defined: point and directional lighting, ambient occlusion, and an approximation of global illumination.

Point lights are computed using a shader invocation for each voxel. The inputs to this shader are light position, initial light intensity, decay power, and distance power. The light position is used to compute the vector along which the light will travel, as well as serve as the origin point for that ray. There is code to handle the cases in which the light is inside the voxel under consideration or if the light position falls outside the volume.

Similar to how the display function operates, there is a process of sampling in discrete steps. In this case, it is done forwards, rather than backwards, and tracks the ray intensity as it is attenuated from a user defined initial value. The value of decay power determines how much the intensity of the ray is attenuated by the alpha value at sampled points along the ray. This also can be expressed as a recurrence relation:
\begin{equation*}
Intensity_{n+1} = Intensity_{n}\cdot(1-alpha^{decay power})
\end{equation*}

This process is repeated until the ray reaches or passes the location of the current voxel. The value of distance power is then used to compute a scale factor that approximates the inverse square law, at the default value of 2.0. It can be set to zero to have the scale factor go to one, or raised to a higher power to decay more in a shorter distance. The final result of the computation is then added to the existing value in voxel in the lighting buffer associated with the shader invocation.

Directional lights are computed in a very similar fashion, but have no divergence -- they use a uniform vector for all shader invocations, as if the light was at infinite distance. The inputs are a pair of euler angles and decay power. The euler angles are used to express the direction of the light vector. The shader does a ray-box intersection to determine near and far intersection points, and an almost identical process is followed, from the near intersection point to the voxel's location. Distance is not considered in this computation.

Ambient occlusion is based on a variable radius neighborhood and is invoked per voxel the same as the previous two. There is logic which does a weighted sum of the neighbohring cells' alpha values according to something like a gaussian kernel, and considers a ratio of this versus the maximum possible value. The value in the lighting buffer is written as the existing value times one minus this ratio.

The structure for global illumination is a little different. The original implementation \cite{VoxelAutomataTerrain} was done serially on the CPU, and made some assumptions based on this sequential evaluation. This required a 2D invocation structure, which would go in slices down the y axis. For each voxel in the current slice, vectors to the nine voxels above it are considered\footnote{These are the voxels which share an edge or corner with the top face of the current voxel.}. The inputs are an alpha threshold and a sky intensity. The vectors make discrete steps through the volume, until one of two things happens. If they hit a voxel whose alpha channel is above the threshold, their contribution is established by some portion of the lighting value at that point. If they escape the volume before encountering an opaque enough voxel, their contribution is established by the sky intensity. The contributions of the nine rays are averaged, and the value is added to the existing value in the lighting buffer for this voxel.

The last lighting feature to consider is called 'mash'. In the same way that the display shader will apply the lighting value as a scale factor to the color data, this same operation is destrucively applied to the RGB data stored in the current color buffer. The lighting buffer can then be reset to neutral values, so the appearance of the voxel data is maintained. The primary idea for this feature was to be able to save voxel data with the lighting applied; however, I have found that there are interesting creative applications for this as well.

\subsection{Other Manipulation Functions}
%blurring, shifting, saving, clearing
Most of these features have come from using the editor. After finding something I wish I was able to do, I figured out a way to implement it. Among them are gaussian and box blurs, shifting, loading and saving, clearing and loading operations that will respect the mask, and a few functions to manipulate the mask buffer. The blurs are applied with variable radii and with the option to respect the mask as well. They diffuse color and alpha values into neighboring cells, which can create interesting effects around masked, opaque voxels. Shifting takes integer arguments for each axis, and moves each voxel to a new location, with the option to loop off the edges.

Loading and saving are used to get data to and from the GPU. The file format that I have been using is a simple PNG image, which contains all the slices of the volume data enumerated out one after another, in a very tall image. Using a lossless compressed format allows for small files that still contain all the data\footnote{For a voxel block 256 on an edge, this is a 256 by 65535 image. By comparison, a BMP image with an alpha channel would be more than 50 megabytes where these are rarely more than a few.}. Using some loader code for PNG images \cite{LodePNG}, the format requires almost no processing before being passed directly to OpenGL as 3D texture data.

\subsection{Interface Elements}
%dearImGUI interface, orientation widget
There are several other improvements to the interface which made the editor much easier to use. I have implemented a convenient GUI using dearImGUI \cite{dearImGUI}, which presents all the functions to the user with a tabbed layout to switch between them. It provides input widgets to set parameters and buttons that can be used to call functions. Another of these features is to deal with the fact that it is easy to become disoriented when rotating the block, with respect to where each axis is pointing. To counter this, I put together a small orientation widget rendered with OpenGL geometry that sits down in the corner of the screen and tracks where the x, y, and z basis vectors are pointing\footnote{See the appendix to for an example.}. This is useful so that you can achieve predictable placement of geometry and lights.

%------------------------------------------------

\section{Conclusions and Future Directions}

One of the largest benefits that comes from this voxel representation of space is a constant-time reference to a global representation of all the geometry. This makes it easier to talk about some parts of how light behaves, but lacks the normal data that would be required in order to determine accurately how it would bounce or refract. Another buffer could be added to keep the normal data when you draw new shapes, but that may not correlate well with things like the blurring operations.

The current system has a limitation in that it uses a single channel representation of light intensity. One direction for future develoments is to convert to an RGB representation, to allow for arbitrarily colored lights. Also related to lighting, I have some ideas about cone lights which would be specified similarly to point lights, but with direction and a solid angle, perhaps something to express how it falls off towards the edges of this angle. With a current desktop GPU, the lighting functions are fast enough to support realtime animation. By caching an ambient level in one of the lighting buffers and applying point or directional lights with varying parameters each frame, this could open up more creative possibilities.

There are a few other features for which I have yet to come up with a usable interface. The first is a scriptable interface for the use of things like signed distance fields, which would allow for fractals and other complex shapes to be defined by the user. Another is a copy/paste operation which would take volume data from one location and apply it elsewhere. By using 3D texture samplers, this could be extended to allow for rotation, scaling, mirrored repeats and more.

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template

\bibitem[Lorenson \& Cline, 1987]{MarchingCubes}
Lorenson, W.\& Cline, H. (July, 1987). Marching Cubes: A High-Resolution 3D Surface Construction Algorithm. ACM Computer Graphics, vol. 21, no. 4, pp. 163--169. Retrieved from \url{http://academy.cba.mit.edu/classes/scanning_printing/MarchingCubes.pdf}.

\bibitem[LodePNG]{LodePNG}
Vandevenne, L. LodePNG. \url{https://lodev.org/lodepng/}.
  
\bibitem[dearImGUI]{dearImGUI}
Cornut, O. DearImGUI. \url{https://github.com/ocornut/imgui}.

\bibitem[Panteleev, 2014]{VoxelGI}
Panteleev, A. Practical Real-Time Voxel-Based Global Illumination for Current GPUs. {\em GPU Technology Conference 2014}. Retrieved from \url{https://on-demand.gputechconf.com/gtc/2014/presentations/S4552-rt-voxel-based-global-illumination-gpus.pdf}.
  
\bibitem[Ju et al., 2002]{DualContour}
T. Ju, F. Losasso, S. Schaefer, and J. Warren. (July, 2002). Dual Contouring of Hermite Data. ACM Transactions on Graphics, vol. 21, no. 3, pp. 339--346, (Proceedings of ACM SIGGRAPH2002). Retrieved from \url{https://www.cse.wustl.edu/~taoju/research/dualContour.pdf}.
  
\bibitem[Quilez, 2006]{VolSort}
Quillez, I. (2006). Volumetric Sort.
\url{https://www.iquilezles.org/www/articles/volumesort/volumesort.htm}.

\bibitem[Quilez, 2008]{RWWTT}
Quillez, I. (2008). Rendering Worlds With Two Triangles. {\em NVScene 2008}. Retrieved from \url{https://www.iquilezles.org/www/material/nvscene2008/nvscene2008.htm}.

\bibitem[Mendoza, 2013]{ArbitraryRotation}
Mendoza, N. (2013). GLSL Rotation About an Arbitrary Axis.
\url{http://www.neilmendoza.com/glsl-rotation-about-an-arbitrary-axis/}
 
\bibitem[Khronos, 2012]{ImgLoadStoreWiki}
Image Load/Store. (2019).
\url{https://www.khronos.org/opengl/wiki/Image_Load_Store} 

\bibitem[Williams et al., 2005]{RayBox}
Williams, Barrus, Morley, Shirley. (July, 2005). An Efficient and Robust Ray-Box Intersection Algorithm. ACM SIGGRAPH 2005 Courses. Retrieved from \url{https://dl.acm.org/doi/10.1145/1198555.1198748} 

\bibitem[Werness, 2017]{VoxelAutomataTerrain}\footnote{I wish to express appreciation to Brent Werness for helping me understand his Voxel Automata Terrain and fake Global Illumination code in order to convert it for my application.}
Werness, B. (April, 2017). Voxel Automata Terrain.
\url{https://bitbucket.org/BWerness/voxel-automata-terrain/src/master/} 

\bibitem[Porter \& Duff, 1984]{AlphaBlending}
Duff, T. \& Porter, T. (July, 1984). Compositing Digital Images. {\em Computer Graphics}, Volume 18, Number 3, 253-259. Retrieved from \url{https://keithp.com/~keithp/porterduff/p253-porter.pdf}  
  
\end{thebibliography}

%----------------------------------------------------------------------------------------
\onecolumn
\section{Appendix: Images}

\begin{figure}[h]
\includegraphics[width=\linewidth]{editor}
Showing the editor interface, with the orientation widget in the lower right corner.
\end{figure}

\begin{figure}[h]
\includegraphics[width=\linewidth]{examples}
Showing some example data made with the editor and lighting functions.
\end{figure}


\end{document}
